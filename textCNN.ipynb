{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score                                                                                                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "import nltk\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from imblearn.over_sampling import SMOTE #upsampleing SMOTE\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from torchsummary import summary \n",
    "\n",
    "\n",
    "\n",
    "train_path = 'input/start-kit/offenseval-training-vB.csv'\n",
    "test_path = 'input/testB/testset-taskb.tsv'\n",
    "embed_path = 'input/embedding/glove.840B.300d.txt'\n",
    "submission_path = 'input/submissionB.csv'\n",
    "model_path = 'model/task2_model.pkl'\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = 'cuda:0'\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "def save_model(model, model_path):\n",
    "    \"\"\"Save model.\"\"\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "\n",
    "def load_model(model, model_path, use_cuda=False):\n",
    "    \"\"\"Load model.\"\"\"\n",
    "    map_location = 'cpu'\n",
    "    if use_cuda and torch.cuda.is_available():\n",
    "        map_location = 'cuda:0'\n",
    "    model.load_state_dict(torch.load(model_path, map_location))\n",
    "    return model\n",
    "\n",
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords.add('@USER')\n",
    "stopWords.add('#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = torch.randn((32,3))\n",
    "# print(y_pred)\n",
    "# y_pred = torch.max(y_pred, 1)[1].data.cpu().numpy()\n",
    "# print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subtask_a\n",
      "0    8840\n",
      "1    4400\n",
      "dtype: int64\n",
      "subtask_b\n",
      "0     524\n",
      "1    3876\n",
      "dtype: int64\n",
      "subtask_c\n",
      "0    2407\n",
      "1    1074\n",
      "2     395\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# #labels change to int,for taskA\n",
    "# path = 'input/start-kit/offenseval-training-v1.tsv'\n",
    "# train=pd.read_csv(path, sep='\\t', header=0)\n",
    "# class_mapping1 = {'NOT':0, 'OFF':1}\n",
    "# class_mapping2 = {'UNT':0, 'TIN':1}\n",
    "# class_mapping3 = {'IND':0, 'GRP':1,'OTH':2}\n",
    "# train['subtask_a'] = train['subtask_a'].map(class_mapping1)\n",
    "# train['subtask_b'] = train['subtask_b'].map(class_mapping2)\n",
    "# train['subtask_c'] = train['subtask_c'].map(class_mapping3)\n",
    "# train[['subtask_b', 'subtask_c']] = train[['subtask_b', 'subtask_c']].fillna(-1)\n",
    "# train[['subtask_b', 'subtask_c']] = train[['subtask_b', 'subtask_c']].astype(int, errors='ignore')\n",
    "# train.to_csv('input/start-kit/offenseval-training-vA.csv',index=0) #不保存行索引\n",
    "\n",
    "# #for taskB\n",
    "# path = 'input/start-kit/offenseval-training-vA.csv'\n",
    "# train=pd.read_csv(path, header=0)\n",
    "# taskB_train = train.loc[train['subtask_a'] == 1]\n",
    "# taskB_train.to_csv('input/start-kit/offenseval-training-vB.csv',index=0) #不保存行索引\n",
    "\n",
    "# #for taskC\n",
    "# path = 'input/start-kit/offenseval-training-vB.csv'\n",
    "# train=pd.read_csv(path, header=0)\n",
    "# taskC_train = train.loc[train['subtask_b'] == 1]\n",
    "# taskC_train.to_csv('input/start-kit/offenseval-training-vC.csv',index=0) #不保存行索引\n",
    "\n",
    "path = 'input/start-kit/offenseval-training-vA.csv'\n",
    "train=pd.read_csv(path, header=0)\n",
    "print(train.groupby(by='subtask_a').size())\n",
    "\n",
    "\n",
    "path = 'input/start-kit/offenseval-training-vB.csv'\n",
    "train=pd.read_csv(path, header=0)\n",
    "print(train.groupby(by='subtask_b').size())\n",
    "\n",
    "path = 'input/start-kit/offenseval-training-vC.csv'\n",
    "train=pd.read_csv(path, header=0)\n",
    "print(train.groupby(by='subtask_c').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_balance(X, y):\n",
    "    '''\n",
    "    :param X: input\n",
    "    :param y: label\n",
    "    :return: balacend X,y\n",
    "    '''\n",
    "    model_smote = SMOTE() \n",
    "    x_smote_resampled, y_smote_resampled = model_smote.fit_resample(X, y) \n",
    "    return x_smote_resampled, y_smote_resampled \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre():\n",
    "    \"\"\"Pre-process model.\"\"\"\n",
    "\n",
    "    print(\"Pre-processing...\")\n",
    "\n",
    "    # load data\n",
    "    fix_length = 48\n",
    "    text = torchtext.data.Field(\n",
    "        sequential=True, use_vocab=True, lower=True,\n",
    "        tokenize=nltk.word_tokenize, batch_first=True,\n",
    "        is_target=False, fix_length=fix_length)\n",
    "    \n",
    "    \n",
    "    \n",
    "    targetA = torchtext.data.Field(\n",
    "        sequential=False, use_vocab=False,\n",
    "        batch_first=True, is_target=True)\n",
    "    \n",
    "    targetB = torchtext.data.Field(\n",
    "    sequential=False, use_vocab=False,\n",
    "    batch_first=True, is_target=True)\n",
    "\n",
    "    targetC = torchtext.data.Field(\n",
    "    sequential=False, use_vocab=False,\n",
    "    batch_first=True, is_target=True)\n",
    "    \n",
    "    \n",
    "    dataset = torchtext.data.TabularDataset(\n",
    "        train_path, format='csv',\n",
    "        fields={\n",
    "                \"tweet\": ('text', text),\n",
    "#                 \"subtask_a\": ('target', targetA),\n",
    "                \"subtask_b\": ('target', targetB),\n",
    "#                 \"subtask_c\": ('target', targetC),\n",
    "               }\n",
    "    )\n",
    "    \n",
    "    data_test = torchtext.data.TabularDataset(\n",
    "        test_path, format='tsv',\n",
    "        fields={\n",
    "                \"tweet\": ('text', text)\n",
    "               }\n",
    "    )\n",
    "\n",
    "    \n",
    "    # build vocab\n",
    "    text.build_vocab(dataset, data_test, min_freq=3)\n",
    "    \n",
    "#  show the frequency\n",
    "#     vocab = text.vocab\n",
    "#     print(vocab.freqs)\n",
    "    \n",
    "    \n",
    "    text.vocab.load_vectors(torchtext.vocab.Vectors(embed_path))\n",
    "    vocab_size = len(text.vocab.itos)\n",
    "    padding_idx = text.vocab.stoi[text.pad_token]\n",
    "\n",
    "    # split data\n",
    "    data_train, data_val = dataset.split(split_ratio=0.9)\n",
    "    \n",
    "    def clean(example, vocab):\n",
    "        return [word for word in example if word in vocab and word not in stopWords and word.isalpha()]\n",
    "    # train here is the `Dataset` instance obtained through .splits()\n",
    "    for example in data_test:\n",
    "      # assuming field name is 'text'\n",
    "        textFiltered = clean(example.text, text.vocab.stoi)\n",
    "        example.text = textFiltered\n",
    "    \n",
    "    print(\"train set size:\", len(data_train))\n",
    "    print(\"val set size:\", len(data_val))\n",
    "    print(\"test set size:\", len(data_test))\n",
    "    print(\"vocab size:\", len(text.vocab.itos))\n",
    "    print(\"embed shape:\", text.vocab.vectors.shape)\n",
    "    print('')\n",
    "\n",
    "    # # save data\n",
    "    # save_pickle(data_train, pickle_path, 'data_train.pkl')\n",
    "    # save_pickle(data_val, pickle_path, 'data_val.pkl')\n",
    "    # print('')\n",
    "\n",
    "    args_dict = {\n",
    "        \"data_train\": data_train, \"data_val\": data_val,\n",
    "        \"data_test\": data_test, \"vocab_size\": vocab_size,\n",
    "        \"padding_idx\": padding_idx,\"all_data\":dataset}\n",
    "\n",
    "    return args_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-processing...\n",
      "train set size: 3960\n",
      "val set size: 440\n",
      "test set size: 240\n",
      "vocab size: 3357\n",
      "embed shape: torch.Size([3357, 300])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = pre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torchtext.data.dataset.TabularDataset object at 0x7f40b6108198>\n"
     ]
    }
   ],
   "source": [
    "all_data = args['all_data']\n",
    "print(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.Iterator(\n",
    "    dataset=args['data_train'], batch_size=128,\n",
    "    train=False, device=DEVICE, sort=False)\n",
    "\n",
    "valid_iter = torchtext.data.Iterator(\n",
    "    dataset=args['data_val'], batch_size=128,\n",
    "    train=False, device=DEVICE, sort=False)\n",
    "\n",
    "text_list = []\n",
    "target_list = []\n",
    "\n",
    "for batch in train_iter:\n",
    "    text = batch.text\n",
    "    target = batch.target\n",
    "    text_list.append(text.cpu().numpy())\n",
    "    target_list.append(target.cpu().numpy())\n",
    "    \n",
    "for batch in valid_iter:\n",
    "    text = batch.text\n",
    "    target = batch.target\n",
    "    text_list.append(text.cpu().numpy())\n",
    "    target_list.append(target.cpu().numpy())\n",
    "    \n",
    "text_list = np.concatenate(text_list, axis=0)\n",
    "target_list = np.concatenate(target_list, axis=0)\n",
    "\n",
    "b_X,b_y = sample_balance(text_list, target_list)\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    \"\"\"Text classification model by character CNN, the implementation of paper\n",
    "    'Yoon Kim. 2014. Convolution Neural Networks for Sentence\n",
    "    Classification.'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, args):\n",
    "        super(TextCNN, self).__init__()\n",
    "\n",
    "        vocab_size = args[\"vocab_size\"]\n",
    "        pretrained_embed = args[\"pretrained_embed\"]\n",
    "        padding_idx = args[\"padding_idx\"]\n",
    "        self.is_mul =args['is_mul']\n",
    "        num_classes = 1\n",
    "        mul_num_classes = 3\n",
    "        kernel_nums = [100, 100, 100]\n",
    "        kernel_sizes = [3, 4, 5]\n",
    "        embed_dim = 300\n",
    "        hidden_dim = 100\n",
    "        drop_prob = 0.5\n",
    "\n",
    "        # no support for pre-trained embedding currently\n",
    "        if pretrained_embed is None:\n",
    "            self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        else:\n",
    "            self.embed = nn.Embedding.from_pretrained(\n",
    "                pretrained_embed, freeze=False)\n",
    "        self.embed.padding_idx = padding_idx\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(embed_dim, kn, ks)\n",
    "             for kn, ks in zip(kernel_nums, kernel_sizes)])\n",
    "        self.fc = nn.Linear(sum(kernel_nums), hidden_dim)\n",
    "        self.drop = nn.Dropout(drop_prob)\n",
    "        \n",
    "        if not self.is_mul:\n",
    "            self.out = nn.Linear(hidden_dim, num_classes)\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        else:\n",
    "            self.out = nn.Linear(hidden_dim, mul_num_classes)\n",
    "            self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def forward(self, word_seq):\n",
    "        # embed\n",
    "        e = self.drop(self.embed(word_seq))  # [b,msl]->[b,msl,e]\n",
    "\n",
    "        # conv and pool, [b,msl,e]->[b,h,msl]\n",
    "        e = e.transpose(1, 2)  # [b,msl,e]->[b,e,msl]\n",
    "        ps = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(e)  # [b,e,msl]->[b,h,msl-k]\n",
    "            p = F.max_pool1d(c, kernel_size=c.size(-1)).squeeze(-1)  # [b,h]\n",
    "            ps.append(p)\n",
    "        p = torch.cat(ps, dim=1)  # [b,h]\n",
    "\n",
    "        # feed-forward, [b,h]->[b]\n",
    "        f = self.drop(self.fc(p))\n",
    "        if not self.is_mul:\n",
    "            logits = self.out(f).squeeze(-1) # b\n",
    "        else:\n",
    "            logits = self.out(f)# b x mul_class\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, fscore, support = score(y_test, predicted)\n",
    "\n",
    "# predicted = [1,2,3,4,5,1,2,1,1,4,5] \n",
    "# y_test = [1,2,3,4,5,1,2,1,1,4,1]\n",
    "# print('precision: {}'.format(precision))\n",
    "# print('recall: {}'.format(recall))\n",
    "# print('fscore: {}'.format(fscore))\n",
    "# print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predict, truth, threshold=0.33,average = True,metric= 'macro'):\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "\n",
    "    :param predict: list of Tensor\n",
    "    :param truth: list of dict\n",
    "    :param threshold: threshold of positive probability\n",
    "    :return eval_results: dict, format {name: metrics}.\n",
    "    \"\"\"\n",
    "    y_trues, y_preds = [], []\n",
    "#     print('predict:',predict)\n",
    "    for y_true, logit in zip(truth, predict):\n",
    "        y_pred = (torch.sigmoid(logit) > threshold).long().cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "    y_true = np.concatenate(y_trues, axis=0)\n",
    "    y_pred = np.concatenate(y_preds, axis=0)\n",
    "    \n",
    "#     print(y_true)\n",
    "#     print(y_pred)\n",
    "    \n",
    "#     precision = metrics.precision_score(y_true, y_pred, pos_label=1)\n",
    "#     recall = metrics.recall_score(y_true, y_pred, pos_label=1)\n",
    "#     f1 = metrics.f1_score(y_true, y_pred, pos_label=1)\n",
    "    if average:\n",
    "        precision = metrics.precision_score(y_true, y_pred, average = metric)\n",
    "        recall = metrics.recall_score(y_true, y_pred,average = metric)\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average = metric)\n",
    "    else:\n",
    "        precision, recall, f1, support = score(y_true, y_pred)\n",
    "    metrics_dict = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "def multi_evaluate(predict, truth,average = True):\n",
    "    \"\"\"Compute evaluation metrics.\n",
    "\n",
    "    :param predict: list of Tensor\n",
    "    :param truth: list of dict\n",
    "    :param threshold: threshold of positive probability\n",
    "    :return eval_results: dict, format {name: metrics}.\n",
    "    \"\"\"\n",
    "    y_trues, y_preds = [], []\n",
    "    for y_true, logit in zip(truth, predict):\n",
    "        \n",
    "        softmax = nn.Softmax(1)# softmax on each row\n",
    "        y_pred = softmax(logit)\n",
    "        y_pred = torch.max(y_pred, 1)[1].data.cpu().numpy()\n",
    "        y_true = y_true.cpu().numpy()\n",
    "        y_trues.append(y_true)\n",
    "        y_preds.append(y_pred)\n",
    "    y_true = np.concatenate(y_trues, axis=0)\n",
    "    y_pred = np.concatenate(y_preds, axis=0)\n",
    "    \n",
    "#     precision = metrics.precision_score(y_true, y_pred, pos_label=1)\n",
    "#     recall = metrics.recall_score(y_true, y_pred, pos_label=1)\n",
    "#     f1 = metrics.f1_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "    if average:\n",
    "        precision = metrics.precision_score(y_true, y_pred, average = 'macro')\n",
    "        recall = metrics.recall_score(y_true, y_pred,average = 'macro')\n",
    "        f1 = metrics.f1_score(y_true, y_pred, average = 'macro')\n",
    "    else:\n",
    "        precision, recall, f1, support = score(y_true, y_pred)\n",
    "        \n",
    "    metrics_dict = {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    return metrics_dict\n",
    "\n",
    "\n",
    "\n",
    "def print_eval_results(results):\n",
    "    \"\"\"Override this method to support more print formats.\n",
    "    :param results: dict, (str: float) is (metrics name: value)\n",
    "    \"\"\"\n",
    "    return \", \".join(\n",
    "        [str(key) + \"=\" + \"{:.4f}\".format(value)\n",
    "         for key, value in results.items()])\n",
    "\n",
    "\n",
    "def test(network, data_iter, threshold=0.33,average = True,metric = 'macro'):\n",
    "    # transfer model to gpu if available\n",
    "    network = network.to(DEVICE)\n",
    "\n",
    "    # turn on the testing mode; clean up the history\n",
    "    network.eval()\n",
    "    output_list = []\n",
    "    truth_list = []\n",
    "    \n",
    "    # predict\n",
    "    for batch in data_iter:\n",
    "        text, target = batch.text, batch.target\n",
    "        text = text.cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prediction = network(text)\n",
    "\n",
    "        output_list.append(prediction.detach())\n",
    "        truth_list.append(target.detach())\n",
    "        \n",
    "    # evaluate\n",
    "    eval_results = evaluate(output_list, truth_list, threshold,average,metric)\n",
    "#     eval_results = multi_evaluate(output_list, truth_list,average)\n",
    "    if average:\n",
    "        print(\"[tester] {}\".format(print_eval_results(eval_results)))\n",
    "\n",
    "    return eval_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, model, loss_func, optimizer, train_iter,valid_iter,show_every = 1000):\n",
    "    start = time.time()\n",
    "    best_f1_score = 0\n",
    "    for e in range(epoch):\n",
    "        model.train()\n",
    "        for step,batch in enumerate(train_iter):\n",
    "            text, target = batch.text, batch.target\n",
    "            text = text.cuda()\n",
    "            optimizer.zero_grad()\n",
    "#             print(text.shape)\n",
    "            logits = model(text)\n",
    "#             print(logits)\n",
    "#             print(target)\n",
    "            loss = model.loss(logits, target.float())\n",
    "#             loss = model.loss(logits, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if step % show_every == 0:\n",
    "                end = time.time()\n",
    "                diff = timedelta(seconds=round(end - start))\n",
    "                print_output = \"[epoch: {:>3} step: {:>4}]\" \\\n",
    "                    \" train loss: {:>4.6} time: {}\".format(\n",
    "                        e, step, loss.item(), diff)\n",
    "                print(print_output)\n",
    "        \n",
    "        \n",
    "        #show F1 on validation and save model if it is the best model so far\n",
    "        eval_results = test(model, valid_iter)\n",
    "        if eval_results['f1'] > best_f1_score:\n",
    "            best_f1_score = eval_results['f1']\n",
    "            save_model(model, model_path)\n",
    "            print(\"**Saved better model selected by validation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = pre()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch:   0 step:    0] train loss: 0.608141 time: 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fl4918/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/fl4918/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "**Saved better model selected by validation.\n",
      "[epoch:   1 step:    0] train loss: 0.484319 time: 0:00:01\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   2 step:    0] train loss: 0.422223 time: 0:00:02\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   3 step:    0] train loss: 0.400204 time: 0:00:02\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   4 step:    0] train loss: 0.350895 time: 0:00:03\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   5 step:    0] train loss: 0.363688 time: 0:00:04\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   6 step:    0] train loss: 0.392987 time: 0:00:04\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   7 step:    0] train loss: 0.314992 time: 0:00:05\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   8 step:    0] train loss: 0.364011 time: 0:00:06\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:   9 step:    0] train loss: 0.424044 time: 0:00:06\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  10 step:    0] train loss: 0.386461 time: 0:00:07\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  11 step:    0] train loss: 0.394388 time: 0:00:07\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  12 step:    0] train loss: 0.290591 time: 0:00:08\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  13 step:    0] train loss: 0.323287 time: 0:00:09\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  14 step:    0] train loss: 0.326456 time: 0:00:10\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  15 step:    0] train loss: 0.293359 time: 0:00:10\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  16 step:    0] train loss: 0.325512 time: 0:00:11\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "[epoch:  17 step:    0] train loss: 0.231229 time: 0:00:12\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "[epoch:  18 step:    0] train loss: 0.257416 time: 0:00:13\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "[epoch:  19 step:    0] train loss: 0.414013 time: 0:00:13\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "[epoch:  20 step:    0] train loss: 0.299059 time: 0:00:14\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  21 step:    0] train loss: 0.395286 time: 0:00:14\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "[epoch:  22 step:    0] train loss: 0.204145 time: 0:00:15\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "[epoch:  23 step:    0] train loss: 0.33163 time: 0:00:16\n",
      "[tester] precision=0.6079, recall=0.5137, f1=0.5004\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  24 step:    0] train loss: 0.376933 time: 0:00:17\n",
      "[tester] precision=0.5840, recall=0.5124, f1=0.4992\n",
      "[epoch:  25 step:    0] train loss: 0.329612 time: 0:00:17\n",
      "[tester] precision=0.6906, recall=0.5081, f1=0.4861\n",
      "[epoch:  26 step:    0] train loss: 0.296619 time: 0:00:18\n",
      "[tester] precision=0.6906, recall=0.5081, f1=0.4861\n",
      "[epoch:  27 step:    0] train loss: 0.278196 time: 0:00:19\n",
      "[tester] precision=0.6906, recall=0.5081, f1=0.4861\n",
      "[epoch:  28 step:    0] train loss: 0.247242 time: 0:00:20\n",
      "[tester] precision=0.6414, recall=0.5150, f1=0.5016\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  29 step:    0] train loss: 0.227158 time: 0:00:20\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "[epoch:  30 step:    0] train loss: 0.320038 time: 0:00:21\n",
      "[tester] precision=0.5654, recall=0.5056, f1=0.4841\n",
      "[epoch:  31 step:    0] train loss: 0.311355 time: 0:00:22\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "[epoch:  32 step:    0] train loss: 0.288745 time: 0:00:22\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "[epoch:  33 step:    0] train loss: 0.289729 time: 0:00:23\n",
      "[tester] precision=0.5654, recall=0.5056, f1=0.4841\n",
      "[epoch:  34 step:    0] train loss: 0.25593 time: 0:00:24\n",
      "[tester] precision=0.5654, recall=0.5056, f1=0.4841\n",
      "[epoch:  35 step:    0] train loss: 0.334552 time: 0:00:24\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "[epoch:  36 step:    0] train loss: 0.242869 time: 0:00:25\n",
      "[tester] precision=0.6915, recall=0.5163, f1=0.5029\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  37 step:    0] train loss: 0.266497 time: 0:00:26\n",
      "[tester] precision=0.6523, recall=0.5381, f1=0.5432\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  38 step:    0] train loss: 0.227842 time: 0:00:26\n",
      "[tester] precision=0.6757, recall=0.5476, f1=0.5577\n",
      "**Saved better model selected by validation.\n",
      "[epoch:  39 step:    0] train loss: 0.265264 time: 0:00:27\n",
      "[tester] precision=0.6713, recall=0.5394, f1=0.5450\n",
      "[epoch:  40 step:    0] train loss: 0.236425 time: 0:00:28\n",
      "[tester] precision=0.6247, recall=0.5287, f1=0.5282\n",
      "[epoch:  41 step:    0] train loss: 0.252404 time: 0:00:28\n",
      "[tester] precision=0.6087, recall=0.5205, f1=0.5142\n",
      "[epoch:  42 step:    0] train loss: 0.189735 time: 0:00:29\n",
      "[tester] precision=0.6430, recall=0.5300, f1=0.5298\n",
      "[epoch:  43 step:    0] train loss: 0.148691 time: 0:00:30\n",
      "[tester] precision=0.6523, recall=0.5381, f1=0.5432\n",
      "[epoch:  44 step:    0] train loss: 0.193246 time: 0:00:30\n",
      "[tester] precision=0.5781, recall=0.5180, f1=0.5113\n",
      "[epoch:  45 step:    0] train loss: 0.187153 time: 0:00:31\n",
      "[tester] precision=0.6102, recall=0.5342, f1=0.5378\n",
      "[epoch:  46 step:    0] train loss: 0.147256 time: 0:00:32\n",
      "[tester] precision=0.5919, recall=0.5193, f1=0.5127\n",
      "[epoch:  47 step:    0] train loss: 0.141464 time: 0:00:32\n",
      "[tester] precision=0.5568, recall=0.5154, f1=0.5086\n",
      "[epoch:  48 step:    0] train loss: 0.118862 time: 0:00:33\n",
      "[tester] precision=0.5853, recall=0.5248, f1=0.5234\n",
      "[epoch:  49 step:    0] train loss: 0.139471 time: 0:00:34\n",
      "[tester] precision=0.5485, recall=0.5141, f1=0.5073\n",
      "[epoch:  50 step:    0] train loss: 0.1413 time: 0:00:35\n",
      "[tester] precision=0.5781, recall=0.5180, f1=0.5113\n",
      "[epoch:  51 step:    0] train loss: 0.122834 time: 0:00:35\n",
      "[tester] precision=0.5853, recall=0.5248, f1=0.5234\n",
      "[epoch:  52 step:    0] train loss: 0.15434 time: 0:00:36\n",
      "[tester] precision=0.5757, recall=0.5235, f1=0.5219\n",
      "[epoch:  53 step:    0] train loss: 0.10315 time: 0:00:36\n",
      "[tester] precision=0.5597, recall=0.5209, f1=0.5189\n",
      "[epoch:  54 step:    0] train loss: 0.127199 time: 0:00:37\n",
      "[tester] precision=0.5757, recall=0.5235, f1=0.5219\n",
      "[epoch:  55 step:    0] train loss: 0.097232 time: 0:00:38\n",
      "[tester] precision=0.5568, recall=0.5154, f1=0.5086\n",
      "[epoch:  56 step:    0] train loss: 0.1149 time: 0:00:38\n",
      "[tester] precision=0.5238, recall=0.5059, f1=0.4933\n",
      "[epoch:  57 step:    0] train loss: 0.0689404 time: 0:00:39\n",
      "[tester] precision=0.5672, recall=0.5222, f1=0.5204\n",
      "[epoch:  58 step:    0] train loss: 0.0959883 time: 0:00:40\n",
      "[tester] precision=0.5634, recall=0.5321, f1=0.5358\n",
      "[epoch:  59 step:    0] train loss: 0.0839895 time: 0:00:40\n",
      "[tester] precision=0.5485, recall=0.5141, f1=0.5073\n",
      "[epoch:  60 step:    0] train loss: 0.135678 time: 0:00:41\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  61 step:    0] train loss: 0.0690548 time: 0:00:42\n",
      "[tester] precision=0.5757, recall=0.5235, f1=0.5219\n",
      "[epoch:  62 step:    0] train loss: 0.0459911 time: 0:00:42\n",
      "[tester] precision=0.5238, recall=0.5059, f1=0.4933\n",
      "[epoch:  63 step:    0] train loss: 0.0845076 time: 0:00:43\n",
      "[tester] precision=0.5562, recall=0.5252, f1=0.5263\n",
      "[epoch:  64 step:    0] train loss: 0.0489491 time: 0:00:44\n",
      "[tester] precision=0.5412, recall=0.5128, f1=0.5059\n",
      "[epoch:  65 step:    0] train loss: 0.0526667 time: 0:00:44\n",
      "[tester] precision=0.5323, recall=0.5145, f1=0.5117\n",
      "[epoch:  66 step:    0] train loss: 0.0497467 time: 0:00:45\n",
      "[tester] precision=0.5634, recall=0.5321, f1=0.5358\n",
      "[epoch:  67 step:    0] train loss: 0.0747444 time: 0:00:46\n",
      "[tester] precision=0.5757, recall=0.5235, f1=0.5219\n",
      "[epoch:  68 step:    0] train loss: 0.0824713 time: 0:00:46\n",
      "[tester] precision=0.5746, recall=0.5291, f1=0.5311\n",
      "[epoch:  69 step:    0] train loss: 0.088714 time: 0:00:47\n",
      "[tester] precision=0.5679, recall=0.5278, f1=0.5295\n",
      "[epoch:  70 step:    0] train loss: 0.0922451 time: 0:00:47\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n",
      "[epoch:  71 step:    0] train loss: 0.0642439 time: 0:00:48\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tester] precision=0.5568, recall=0.5154, f1=0.5086\n",
      "[epoch:  72 step:    0] train loss: 0.0505355 time: 0:00:49\n",
      "[tester] precision=0.5597, recall=0.5209, f1=0.5189\n",
      "[epoch:  73 step:    0] train loss: 0.0629438 time: 0:00:49\n",
      "[tester] precision=0.5679, recall=0.5278, f1=0.5295\n",
      "[epoch:  74 step:    0] train loss: 0.0207552 time: 0:00:50\n",
      "[tester] precision=0.5562, recall=0.5252, f1=0.5263\n",
      "[epoch:  75 step:    0] train loss: 0.054356 time: 0:00:51\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  76 step:    0] train loss: 0.0316518 time: 0:00:51\n",
      "[tester] precision=0.5417, recall=0.5171, f1=0.5145\n",
      "[epoch:  77 step:    0] train loss: 0.0244399 time: 0:00:52\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  78 step:    0] train loss: 0.043176 time: 0:00:53\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n",
      "[epoch:  79 step:    0] train loss: 0.0393303 time: 0:00:54\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  80 step:    0] train loss: 0.0294195 time: 0:00:55\n",
      "[tester] precision=0.5562, recall=0.5252, f1=0.5263\n",
      "[epoch:  81 step:    0] train loss: 0.0560892 time: 0:00:55\n",
      "[tester] precision=0.5672, recall=0.5222, f1=0.5204\n",
      "[epoch:  82 step:    0] train loss: 0.0338645 time: 0:00:56\n",
      "[tester] precision=0.5417, recall=0.5171, f1=0.5145\n",
      "[epoch:  83 step:    0] train loss: 0.0585745 time: 0:00:57\n",
      "[tester] precision=0.5511, recall=0.5239, f1=0.5247\n",
      "[epoch:  84 step:    0] train loss: 0.0418708 time: 0:00:57\n",
      "[tester] precision=0.5562, recall=0.5252, f1=0.5263\n",
      "[epoch:  85 step:    0] train loss: 0.0609378 time: 0:00:58\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n",
      "[epoch:  86 step:    0] train loss: 0.0457927 time: 0:00:59\n",
      "[tester] precision=0.5465, recall=0.5226, f1=0.5232\n",
      "[epoch:  87 step:    0] train loss: 0.0860095 time: 0:01:00\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n",
      "[epoch:  88 step:    0] train loss: 0.0295252 time: 0:01:00\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  89 step:    0] train loss: 0.00882964 time: 0:01:01\n",
      "[tester] precision=0.5586, recall=0.5308, f1=0.5341\n",
      "[epoch:  90 step:    0] train loss: 0.0133817 time: 0:01:02\n",
      "[tester] precision=0.5465, recall=0.5226, f1=0.5232\n",
      "[epoch:  91 step:    0] train loss: 0.0376341 time: 0:01:02\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  92 step:    0] train loss: 0.0324415 time: 0:01:03\n",
      "[tester] precision=0.5417, recall=0.5171, f1=0.5145\n",
      "[epoch:  93 step:    0] train loss: 0.0124822 time: 0:01:04\n",
      "[tester] precision=0.5417, recall=0.5171, f1=0.5145\n",
      "[epoch:  94 step:    0] train loss: 0.0295334 time: 0:01:04\n",
      "[tester] precision=0.5417, recall=0.5171, f1=0.5145\n",
      "[epoch:  95 step:    0] train loss: 0.0370235 time: 0:01:05\n",
      "[tester] precision=0.5471, recall=0.5184, f1=0.5160\n",
      "[epoch:  96 step:    0] train loss: 0.0185214 time: 0:01:05\n",
      "[tester] precision=0.5542, recall=0.5295, f1=0.5325\n",
      "[epoch:  97 step:    0] train loss: 0.0147773 time: 0:01:06\n",
      "[tester] precision=0.5511, recall=0.5239, f1=0.5247\n",
      "[epoch:  98 step:    0] train loss: 0.0261402 time: 0:01:07\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n",
      "[epoch:  99 step:    0] train loss: 0.0113776 time: 0:01:07\n",
      "[tester] precision=0.5618, recall=0.5265, f1=0.5279\n"
     ]
    }
   ],
   "source": [
    "data_train = args[\"data_train\"]\n",
    "data_val = args[\"data_val\"]\n",
    "pretrained_embed = data_train.fields[\"text\"].vocab.vectors\n",
    "\n",
    "# define model\n",
    "model_args = {\n",
    "    \"vocab_size\": args[\"vocab_size\"],\n",
    "    \"padding_idx\": args[\"padding_idx\"],\n",
    "    \"pretrained_embed\": pretrained_embed,\n",
    "    \"is_mul\":False\n",
    "}\n",
    "\n",
    "\n",
    "# define batch iterator\n",
    "train_iter = torchtext.data.Iterator(\n",
    "    dataset=data_train, batch_size=batch_size,\n",
    "    train=True, shuffle=True, sort=False,\n",
    "    device=DEVICE)\n",
    "\n",
    "# define batch iterator\n",
    "valid_iter = torchtext.data.Iterator(\n",
    "    dataset=data_val, batch_size=batch_size,\n",
    "    train=False, device=DEVICE, sort=False)\n",
    "\n",
    "model = TextCNN(model_args).cuda()\n",
    "# print(model)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4,betas=(0.5, 0.999))\n",
    "\n",
    "\n",
    "train(100,model,loss_func,optimizer,train_iter,valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "model_args = {\n",
    "    \"vocab_size\": args[\"vocab_size\"],\n",
    "    \"padding_idx\": args[\"padding_idx\"],\n",
    "    \"pretrained_embed\": None,\n",
    "    \"is_mul\":False\n",
    "}\n",
    "model = TextCNN(model_args)\n",
    "load_model(model, model_path, use_cuda=True)\n",
    "\n",
    "def parmTuning(args,valid_iter):\n",
    "    # test and threshold selection\n",
    "    data_val = args[\"data_val\"]\n",
    "    best_thresh, best_f1 = 0., 0.\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        f1 = test(model, valid_iter, threshold=thresh,average = True)[\"f1\"]\n",
    "        print(\"threshold: {:>.2f} f1: {:>.4f}\".format(thresh, f1))\n",
    "        if f1 > best_f1:\n",
    "            best_thresh, best_f1 = thresh, f1\n",
    "\n",
    "    args[\"threshold\"] = best_thresh\n",
    "\n",
    "\n",
    "    print(\"best f1 on dev: {:>.4f} threshold: {:>.2f}\".format(\n",
    "        best_f1, best_thresh))\n",
    "    \n",
    "        \n",
    "    metrics = test(model, valid_iter, threshold=best_thresh,average = False)\n",
    "    print(metrics)\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fl4918/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/fl4918/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "threshold: 0.10 f1: 0.4680\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "threshold: 0.11 f1: 0.4680\n",
      "[tester] precision=0.4398, recall=0.5000, f1=0.4680\n",
      "threshold: 0.12 f1: 0.4680\n",
      "[tester] precision=0.4396, recall=0.4987, f1=0.4673\n",
      "threshold: 0.13 f1: 0.4673\n",
      "[tester] precision=0.4396, recall=0.4987, f1=0.4673\n",
      "threshold: 0.14 f1: 0.4673\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.15 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.16 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.17 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.18 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.19 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.20 f1: 0.4667\n",
      "[tester] precision=0.4395, recall=0.4974, f1=0.4667\n",
      "threshold: 0.21 f1: 0.4667\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "threshold: 0.22 f1: 0.4851\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "threshold: 0.23 f1: 0.4851\n",
      "[tester] precision=0.6072, recall=0.5068, f1=0.4851\n",
      "threshold: 0.24 f1: 0.4851\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "threshold: 0.25 f1: 0.4832\n",
      "[tester] precision=0.5402, recall=0.5043, f1=0.4832\n",
      "threshold: 0.26 f1: 0.4832\n",
      "[tester] precision=0.5234, recall=0.5030, f1=0.4822\n",
      "threshold: 0.27 f1: 0.4822\n",
      "[tester] precision=0.5660, recall=0.5111, f1=0.4980\n",
      "threshold: 0.28 f1: 0.4980\n",
      "[tester] precision=0.5660, recall=0.5111, f1=0.4980\n",
      "threshold: 0.29 f1: 0.4980\n",
      "[tester] precision=0.6087, recall=0.5205, f1=0.5142\n",
      "threshold: 0.30 f1: 0.5142\n",
      "[tester] precision=0.6713, recall=0.5394, f1=0.5450\n",
      "threshold: 0.31 f1: 0.5450\n",
      "[tester] precision=0.6713, recall=0.5394, f1=0.5450\n",
      "threshold: 0.32 f1: 0.5450\n",
      "[tester] precision=0.6757, recall=0.5476, f1=0.5577\n",
      "threshold: 0.33 f1: 0.5577\n",
      "[tester] precision=0.6757, recall=0.5476, f1=0.5577\n",
      "threshold: 0.34 f1: 0.5577\n",
      "[tester] precision=0.6757, recall=0.5476, f1=0.5577\n",
      "threshold: 0.35 f1: 0.5577\n",
      "[tester] precision=0.6757, recall=0.5476, f1=0.5577\n",
      "threshold: 0.36 f1: 0.5577\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "args = parmTuning(args,valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    \"\"\"An interface for predicting outputs based on trained models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=8, use_cuda=False):\n",
    "        self.batch_size = batch_size\n",
    "        self.use_cuda = use_cuda\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available() and self.use_cuda:\n",
    "            self.device = 'cuda:0'\n",
    "\n",
    "    def predict(self, network, data, threshold=0.33,mul = False):\n",
    "        # transfer model to gpu if available\n",
    "        network = network.to(self.device)\n",
    "\n",
    "        # turn on the testing mode; clean up the history\n",
    "        network.eval()\n",
    "        batch_output = []\n",
    "\n",
    "        # define batch iterator\n",
    "        data_iter = torchtext.data.Iterator(\n",
    "            dataset=data, batch_size=self.batch_size,\n",
    "            train=False, device=self.device, sort=False)\n",
    "    \n",
    "        for batch in data_iter:\n",
    "            text = batch.text\n",
    "            with torch.no_grad():\n",
    "                prediction = network(text)\n",
    "            batch_output.append(prediction.detach())\n",
    "\n",
    "        if mul:\n",
    "            return self.softmax_processor(batch_output)\n",
    "        else:\n",
    "            return self._post_processor(batch_output, threshold)\n",
    "\n",
    "    def _post_processor(self, batch_output, threshold=0.33):\n",
    "        \"\"\"Convert logit tensor to label.\"\"\"\n",
    "        y_preds = []\n",
    "        for logit in batch_output:\n",
    "            y_pred = (torch.sigmoid(logit) > threshold).long().cpu().numpy()\n",
    "            y_preds.append(y_pred)\n",
    "        y_pred = np.concatenate(y_preds, axis=0)\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def softmax_processor(self,batch_output):\n",
    "        y_preds = []\n",
    "        for logit in batch_output:\n",
    "            softmax = nn.Softmax(1)# softmax on each row\n",
    "            y_pred = softmax(logit)\n",
    "            y_pred = torch.max(y_pred, 1)[1].data.cpu().numpy()\n",
    "            y_preds.append(y_pred)\n",
    "        y_pred = np.concatenate(y_preds, axis=0)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictor\n",
    "predictor = Predictor(batch_size=128, use_cuda=True)\n",
    "\n",
    "# define model\n",
    "model_args = {\n",
    "    \"vocab_size\": args[\"vocab_size\"],\n",
    "    \"padding_idx\": args[\"padding_idx\"],\n",
    "    \"pretrained_embed\": None,\n",
    "    \"is_mul\":False\n",
    "}\n",
    "model = TextCNN(model_args)\n",
    "load_model(model, model_path, use_cuda=True)\n",
    "\n",
    "\n",
    "# predict\n",
    "data_test = args[\"data_test\"]\n",
    "threshold = args[\"threshold\"]\n",
    "#single:\n",
    "# y_pred = predictor.predict(model, data_test, threshold=threshold,mul = False)\n",
    "#mul\n",
    "y_pred = predictor.predict(model, data_test, threshold=threshold,mul = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit result\n",
    "test_df = pd.read_csv(test_path, sep='\\t', index_col=False, header=0)\n",
    "data = {\"qid\": test_df[\"id\"],\"prediction\": y_pred}\n",
    "print(y_pred)\n",
    "# print(data)\n",
    "subm_df = pd.DataFrame(data=data)\n",
    "\n",
    "data = subm_df['prediction']\n",
    "\n",
    "# class_mapping1 = {'NOT':0, 'OFF':1}\n",
    "# data.loc[data == 0] = 'NOT'\n",
    "# data.loc[data == 1] = 'OFF'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # class_mapping2 = {'UNT':0, 'TIN':1}\n",
    "\n",
    "data.loc[data == 0] = 'UNT'\n",
    "data.loc[data == 1] = 'TIN'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data.loc[data == 0] = 'IND'\n",
    "# data.loc[data == 1] = 'GRP'\n",
    "# data.loc[data == 2] = 'OTH'\n",
    "subm_df.to_csv(submission_path, header=False, index=False)\n",
    "print(\"submission saved as {}.\".format(submission_path))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define predictor\n",
    "predictor = Predictor(batch_size=128, use_cuda=True)\n",
    "\n",
    "# define model\n",
    "model_args = {\n",
    "    \"vocab_size\": args[\"vocab_size\"],\n",
    "    \"padding_idx\": args[\"padding_idx\"],\n",
    "    \"pretrained_embed\": None,\n",
    "    \"is_mul\":False\n",
    "}\n",
    "model = TextCNN(model_args)\n",
    "load_model(model, model_path, use_cuda=True)\n",
    "model = model.cuda()\n",
    "model = model.eval()\n",
    "y_preds = []\n",
    "real_y = []\n",
    "batch_output = []\n",
    "\n",
    "\n",
    "all_data_iter = torchtext.data.Iterator(\n",
    "    dataset=args['all_data'], batch_size=128,\n",
    "    train=False, device=DEVICE, sort=False)\n",
    "\n",
    "for batch in all_data_iter:\n",
    "    text = batch.text\n",
    "    target = batch.target\n",
    "    text = text.cuda()\n",
    "    prediction = model(text)\n",
    "    real_y.append(target)\n",
    "    batch_output.append(prediction.detach())\n",
    "    \n",
    "for logit in batch_output:\n",
    "    #y_pred = (torch.sigmoid(logit) > threshold).long().cpu().numpy()\n",
    "    softmax = nn.Softmax(1)# softmax on each row\n",
    "    y_pred = softmax(logit)\n",
    "    y_pred = torch.max(y_pred, 1)[1].data.cpu().numpy()\n",
    "    y_preds.append(y_pred)\n",
    "y_pred = np.concatenate(y_preds, axis=0)\n",
    "real_y = np.concatenate(real_y, axis=0)\n",
    "print('y_pred:',y_pred)\n",
    "print(Counter(y_pred))\n",
    "print('real_y:',real_y)\n",
    "print(Counter(real_y))\n",
    "print( (y_pred == real_y).sum()/ len(real_y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
